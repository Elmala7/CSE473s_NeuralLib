{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc8d9bb",
   "metadata": {},
   "source": [
    "# Project Demo: Neural Network Library (Part 1)\n",
    "\n",
    "This notebook demonstrates the core functionality of our NumPy-based neural network library by implementing and validating two key tasks:\n",
    "1. Gradient checking to verify backpropagation correctness\n",
    "2. Training a network to solve the XOR problem\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Gradient Checking\n",
    "\n",
    "Demonstrating that the backpropagation implementation matches numerical gradient approximations.\n",
    "\n",
    "The gradient check validates that our analytical gradients (computed via backpropagation) are mathematically equivalent to numerical gradients (computed via finite differences). This is a critical sanity check that proves the backpropagation implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcaac71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Gradient Check -----\n",
      "\n",
      "ðŸ“Š Gradient Comparison:\n",
      "\n",
      "Analytic Gradient (from backprop):\n",
      "[[-2.07469049]\n",
      " [-0.43725958]]\n",
      "\n",
      "Numerical Gradient (finite difference):\n",
      "[[-2.07469049]\n",
      " [-0.43725958]]\n",
      "\n",
      "Element-wise difference:\n",
      "[[ 1.13935528e-11]\n",
      " [-3.50836027e-12]]\n",
      "\n",
      "ðŸ“ˆ Relative Difference (norm-based): 2.81e-12\n",
      "âœ… Gradient check PASSED!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from lib.layers import Dense\n",
    "from lib.activations import Tanh, Sigmoid\n",
    "from lib.losses import MSE\n",
    "from lib.network import Sequential\n",
    "\n",
    "def gradient_check():\n",
    "    print(\"----- Gradient Check -----\")\n",
    "    # Setup\n",
    "    np.random.seed(15)  # For reproducibility\n",
    "    x_sample = np.random.rand(1, 2)\n",
    "    y_sample = np.array([[1]])\n",
    "    layer = Dense(2, 1)\n",
    "    loss_func = MSE()\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    # Get analytic gradient\n",
    "    output = layer.forward(x_sample)\n",
    "    loss_func.loss(y_sample, output)\n",
    "    error_grad = loss_func.loss_prime(y_sample, output)\n",
    "    layer.backward(error_grad)  # No learning_rate parameter\n",
    "    analytic_grad = layer.grad_weights.copy()\n",
    "\n",
    "    # Get numerical gradient\n",
    "    numerical_grad = np.zeros_like(layer.weights)\n",
    "    it = np.nditer(layer.weights, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        # J(W + e)\n",
    "        original_w = layer.weights[ix]\n",
    "        layer.weights[ix] = original_w + epsilon\n",
    "        output_plus = layer.forward(x_sample)\n",
    "        J_plus = loss_func.loss(y_sample, output_plus)\n",
    "\n",
    "        # J(W - e)\n",
    "        layer.weights[ix] = original_w - epsilon\n",
    "        output_minus = layer.forward(x_sample)\n",
    "        J_minus = loss_func.loss(y_sample, output_minus)\n",
    "        \n",
    "        # (J(W+e) - J(W-e)) / 2e\n",
    "        numerical_grad[ix] = (J_plus - J_minus) / (2 * epsilon)\n",
    "        layer.weights[ix] = original_w  # Restore\n",
    "        it.iternext()\n",
    "    \n",
    "    # Print gradients\n",
    "    print(\"\\nðŸ“Š Gradient Comparison:\")\n",
    "    print(f\"\\nAnalytic Gradient (from backprop):\\n{analytic_grad}\")\n",
    "    print(f\"\\nNumerical Gradient (finite difference):\\n{numerical_grad}\")\n",
    "    print(f\"\\nElement-wise difference:\\n{analytic_grad - numerical_grad}\")\n",
    "    \n",
    "    # Compare\n",
    "    diff = np.linalg.norm(analytic_grad - numerical_grad) / np.linalg.norm(analytic_grad + numerical_grad)\n",
    "    print(f\"\\nðŸ“ˆ Relative Difference (norm-based): {diff:.2e}\")\n",
    "    assert diff < 1e-4, \"Gradient check failed!\"\n",
    "    print(\"âœ… Gradient check PASSED!\")\n",
    "\n",
    "gradient_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f876e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: The XOR Problem\n",
    "\n",
    "Training a 2-layer network to solve the non-linear XOR gate with 100% accuracy.\n",
    "\n",
    "The XOR problem is a classic benchmark for neural networks. It's non-linearly separable, meaning a single linear layer cannot solve it. We use a 2-layer network with Tanh activation to demonstrate the power of deep learning for solving non-linear problems.\n",
    "\n",
    "**Network Architecture:** Input (2) â†’ Dense(16) â†’ Tanh â†’ Dense(1) â†’ Sigmoid â†’ Output\n",
    "\n",
    "**Training Configuration:**\n",
    "- Learning Rate: 1.0\n",
    "- Epochs: 10,000\n",
    "- Weight Initialization: He initialization\n",
    "- Optimizer: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4345319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Problem Setup Complete\n",
      "Input shape: (4, 2), Output shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reload modules and setup\n",
    "import sys\n",
    "for module_name in list(sys.modules.keys()):\n",
    "    if module_name.startswith('lib'):\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "from lib.network import Sequential\n",
    "from lib.layers import Dense\n",
    "from lib.activations import Tanh, Sigmoid\n",
    "from lib.losses import MSE\n",
    "\n",
    "np.random.seed(15)  # For reproducibility\n",
    "\n",
    "# XOR Data\n",
    "X = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Dense(2, 16))\n",
    "model.add(Tanh())\n",
    "model.add(Dense(16, 1))\n",
    "model.add(Sigmoid())\n",
    "model.use_loss(MSE())\n",
    "\n",
    "print(\"XOR Problem Setup Complete\")\n",
    "print(f\"Input shape: {X.shape}, Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ca197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XOR Network\n",
      "==================================================\n",
      "Epoch 1/10000   error=0.2566806479918462\n",
      "Epoch 101/10000   error=0.00665254893421432\n",
      "Epoch 201/10000   error=0.002004820285373787\n",
      "Epoch 301/10000   error=0.0010898237808405536\n",
      "Epoch 401/10000   error=0.0007285769746313187\n",
      "Epoch 501/10000   error=0.0005402173130643051\n",
      "Epoch 601/10000   error=0.00042607960490950443\n",
      "Epoch 701/10000   error=0.00035008835030134633\n",
      "Epoch 801/10000   error=0.00029612215840010364\n",
      "Epoch 901/10000   error=0.00025595447074431004\n",
      "Epoch 1001/10000   error=0.00022497046583883366\n",
      "Epoch 1101/10000   error=0.00020039055597013784\n",
      "Epoch 1201/10000   error=0.00018044508717754854\n",
      "Epoch 1301/10000   error=0.00016395614855264725\n",
      "Epoch 1401/10000   error=0.00015011057159447646\n",
      "Epoch 1501/10000   error=0.0001383296170108075\n",
      "Epoch 1601/10000   error=0.00012819060881994856\n",
      "Epoch 1701/10000   error=0.00011937792325631825\n",
      "Epoch 1801/10000   error=0.00011165129074625855\n",
      "Epoch 1901/10000   error=0.00010482469101394992\n",
      "Epoch 2001/10000   error=9.875193981547547e-05\n",
      "Epoch 2101/10000   error=9.331662268629336e-05\n",
      "Epoch 2201/10000   error=8.842492333626588e-05\n",
      "Epoch 2301/10000   error=8.400042252632713e-05\n",
      "Epoch 2401/10000   error=7.998026508717022e-05\n",
      "Epoch 2501/10000   error=7.631229394381701e-05\n",
      "Epoch 2601/10000   error=7.295287875217875e-05\n",
      "Epoch 2701/10000   error=6.986525087811671e-05\n",
      "Epoch 2801/10000   error=6.701821247819769e-05\n",
      "Epoch 2901/10000   error=6.438512541401671e-05\n",
      "Epoch 3001/10000   error=6.194311188228543e-05\n",
      "Epoch 3101/10000   error=5.967241691847641e-05\n",
      "Epoch 3201/10000   error=5.755589587899949e-05\n",
      "Epoch 3301/10000   error=5.557859929536377e-05\n",
      "Epoch 3401/10000   error=5.372743423593859e-05\n",
      "Epoch 3501/10000   error=5.199088625844432e-05\n",
      "Epoch 3601/10000   error=5.0358789704017885e-05\n",
      "Epoch 3701/10000   error=4.8822136828798415e-05\n",
      "Epoch 3801/10000   error=4.737291834181472e-05\n",
      "Epoch 3901/10000   error=4.600398949659065e-05\n",
      "Epoch 4001/10000   error=4.470895709554029e-05\n",
      "Epoch 4101/10000   error=4.34820837032235e-05\n",
      "Epoch 4201/10000   error=4.23182060943294e-05\n",
      "Epoch 4301/10000   error=4.1212665534344775e-05\n",
      "Epoch 4401/10000   error=4.0161247942331756e-05\n",
      "Epoch 4501/10000   error=3.9160132343526665e-05\n",
      "Epoch 4601/10000   error=3.820584630551524e-05\n",
      "Epoch 4701/10000   error=3.7295227281359885e-05\n",
      "Epoch 4801/10000   error=3.6425388968252814e-05\n",
      "Epoch 4901/10000   error=3.5593691940502253e-05\n",
      "Epoch 5001/10000   error=3.47977179379911e-05\n",
      "Epoch 5101/10000   error=3.403524729138543e-05\n",
      "Epoch 5201/10000   error=3.3304239047644165e-05\n",
      "Epoch 5301/10000   error=3.260281342733288e-05\n",
      "Epoch 5401/10000   error=3.1929236301464526e-05\n",
      "Epoch 5501/10000   error=3.128190542241985e-05\n",
      "Epoch 5601/10000   error=3.065933818256667e-05\n",
      "Epoch 5701/10000   error=3.0060160706895925e-05\n",
      "Epoch 5801/10000   error=2.9483098113538323e-05\n",
      "Epoch 5901/10000   error=2.89269657991702e-05\n",
      "Epoch 6001/10000   error=2.8390661625983714e-05\n",
      "Epoch 6101/10000   error=2.787315890350898e-05\n",
      "Epoch 6201/10000   error=2.7373500072759993e-05\n",
      "Epoch 6301/10000   error=2.689079101225022e-05\n",
      "Epoch 6401/10000   error=2.6424195895776302e-05\n",
      "Epoch 6501/10000   error=2.5972932540735907e-05\n",
      "Epoch 6601/10000   error=2.5536268193370453e-05\n",
      "Epoch 6701/10000   error=2.5113515703913355e-05\n",
      "Epoch 6801/10000   error=2.4704030050306308e-05\n",
      "Epoch 6901/10000   error=2.4307205174058905e-05\n",
      "Epoch 7001/10000   error=2.3922471096130973e-05\n",
      "Epoch 7101/10000   error=2.3549291284398018e-05\n",
      "Epoch 7201/10000   error=2.3187160247539136e-05\n",
      "Epoch 7301/10000   error=2.283560133299966e-05\n",
      "Epoch 7401/10000   error=2.2494164709165515e-05\n",
      "Epoch 7501/10000   error=2.2162425514056796e-05\n",
      "Epoch 7601/10000   error=2.18399821547761e-05\n",
      "Epoch 7701/10000   error=2.152645474360784e-05\n",
      "Epoch 7801/10000   error=2.1221483658153938e-05\n",
      "Epoch 7901/10000   error=2.0924728214221073e-05\n",
      "Epoch 8001/10000   error=2.063586544129845e-05\n",
      "Epoch 8101/10000   error=2.0354588951526927e-05\n",
      "Epoch 8201/10000   error=2.0080607893951916e-05\n",
      "Epoch 8301/10000   error=1.9813645986665755e-05\n",
      "Epoch 8401/10000   error=1.955344062017356e-05\n",
      "Epoch 8501/10000   error=1.9299742025962575e-05\n",
      "Epoch 8601/10000   error=1.9052312504808387e-05\n",
      "Epoch 8701/10000   error=1.8810925709894513e-05\n",
      "Epoch 8801/10000   error=1.8575365980259926e-05\n",
      "Epoch 8901/10000   error=1.834542772051871e-05\n",
      "Epoch 9001/10000   error=1.812091482314075e-05\n",
      "Epoch 9101/10000   error=1.7901640129949167e-05\n",
      "Epoch 9201/10000   error=1.7687424929761525e-05\n",
      "Epoch 9301/10000   error=1.747809848938599e-05\n",
      "Epoch 9401/10000   error=1.7273497615419786e-05\n",
      "Epoch 9501/10000   error=1.7073466244525973e-05\n",
      "Epoch 9601/10000   error=1.6877855060052325e-05\n",
      "Epoch 9701/10000   error=1.668652113304731e-05\n",
      "Epoch 9801/10000   error=1.64993275858797e-05\n",
      "Epoch 9901/10000   error=1.631614327682713e-05\n",
      "==================================================\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Training XOR Network\")\n",
    "print(\"=\" * 50)\n",
    "model.train(X, y, epochs=10000, learning_rate=1.0)\n",
    "print(\"=\" * 50)\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f50d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL RESULTS\n",
      "==================================================\n",
      "\n",
      "Raw Predictions:\n",
      "  Input [-1. -1.] â†’ 0.004415 â†’ 0\n",
      "  Input [-1.  1.] â†’ 0.996611 â†’ 1\n",
      "  Input [ 1. -1.] â†’ 0.995951 â†’ 1\n",
      "  Input [1. 1.] â†’ 0.004144 â†’ 0\n",
      "\n",
      "Final Prediction Vector: [0. 1. 1. 0.]\n",
      "Expected XOR Output:    [0. 1. 1. 0.]\n",
      "âœ“ MATCH: True\n",
      "\n",
      "Final Loss: 0.000016137\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "pred_values = np.array([p.flatten()[0] for p in predictions])\n",
    "rounded_preds = np.round(pred_values)\n",
    "\n",
    "print(\"\\nRaw Predictions:\")\n",
    "for i, (inp, pred, rounded) in enumerate(zip(X, pred_values, rounded_preds)):\n",
    "    print(f\"  Input {inp} â†’ {pred:.6f} â†’ {int(rounded)}\")\n",
    "\n",
    "print(f\"\\nFinal Prediction Vector: {rounded_preds}\")\n",
    "print(f\"Expected XOR Output:    [0. 1. 1. 0.]\")\n",
    "print(f\"âœ“ MATCH: {np.array_equal(rounded_preds, np.array([0., 1., 1., 0.]))}\")\n",
    "\n",
    "# Calculate final loss\n",
    "final_loss = 0\n",
    "for i in range(len(X)):\n",
    "    output = model.predict([X[i]])[0]\n",
    "    final_loss += model.loss.loss(np.array([[y[i][0]]]), output)\n",
    "final_loss /= len(X)\n",
    "\n",
    "print(f\"\\nFinal Loss: {final_loss:.9f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecha_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
