{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc8d9bb",
   "metadata": {},
   "source": [
    "# Project Demo: Neural Network Library (Part 1)\n",
    "\n",
    "This notebook demonstrates the core functionality of our NumPy-based neural network library by implementing and validating two key tasks:\n",
    "1. Gradient checking to verify backpropagation correctness\n",
    "2. Training a network to solve the XOR problem\n",
    "\n",
    "---\n",
    "\n",
    "## Section 1: Gradient Checking\n",
    "\n",
    "Demonstrating that the backpropagation implementation matches numerical gradient approximations.\n",
    "\n",
    "The gradient check validates that our analytical gradients (computed via backpropagation) are mathematically equivalent to numerical gradients (computed via finite differences). This is a critical sanity check that proves the backpropagation implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaac71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from lib.layers import Dense\n",
    "from lib.activations import Tanh, Sigmoid\n",
    "from lib.losses import MSE\n",
    "from lib.network import Sequential\n",
    "\n",
    "def gradient_check():\n",
    "    print(\"----- Gradient Check -----\")\n",
    "    # Setup\n",
    "    np.random.seed(15)  # For reproducibility\n",
    "    x_sample = np.random.rand(1, 2)\n",
    "    y_sample = np.array([[1]])\n",
    "    layer = Dense(2, 1)\n",
    "    loss_func = MSE()\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    # Get analytic gradient\n",
    "    output = layer.forward(x_sample)\n",
    "    loss_func.loss(y_sample, output)\n",
    "    error_grad = loss_func.loss_prime(y_sample, output)\n",
    "    layer.backward(error_grad, 0)  # learning_rate=0 to avoid weight updates\n",
    "    analytic_grad = layer.grad_weights.copy()\n",
    "\n",
    "    # Get numerical gradient\n",
    "    numerical_grad = np.zeros_like(layer.weights)\n",
    "    it = np.nditer(layer.weights, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        # J(W + e)\n",
    "        original_w = layer.weights[ix]\n",
    "        layer.weights[ix] = original_w + epsilon\n",
    "        output_plus = layer.forward(x_sample)\n",
    "        J_plus = loss_func.loss(y_sample, output_plus)\n",
    "\n",
    "        # J(W - e)\n",
    "        layer.weights[ix] = original_w - epsilon\n",
    "        output_minus = layer.forward(x_sample)\n",
    "        J_minus = loss_func.loss(y_sample, output_minus)\n",
    "        \n",
    "        # (J(W+e) - J(W-e)) / 2e\n",
    "        numerical_grad[ix] = (J_plus - J_minus) / (2 * epsilon)\n",
    "        layer.weights[ix] = original_w  # Restore\n",
    "        it.iternext()\n",
    "    \n",
    "    # Print gradients\n",
    "    print(\"\\nðŸ“Š Gradient Comparison:\")\n",
    "    print(f\"\\nAnalytic Gradient (from backprop):\\n{analytic_grad}\")\n",
    "    print(f\"\\nNumerical Gradient (finite difference):\\n{numerical_grad}\")\n",
    "    print(f\"\\nElement-wise difference:\\n{analytic_grad - numerical_grad}\")\n",
    "    \n",
    "    # Compare\n",
    "    diff = np.linalg.norm(analytic_grad - numerical_grad) / np.linalg.norm(analytic_grad + numerical_grad)\n",
    "    print(f\"\\nðŸ“ˆ Relative Difference (norm-based): {diff:.2e}\")\n",
    "    assert diff < 1e-4, \"Gradient check failed!\"\n",
    "    print(\"âœ… Gradient check PASSED!\")\n",
    "\n",
    "gradient_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f876e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: The XOR Problem\n",
    "\n",
    "Training a 2-layer network to solve the non-linear XOR gate with 100% accuracy.\n",
    "\n",
    "The XOR problem is a classic benchmark for neural networks. It's non-linearly separable, meaning a single linear layer cannot solve it. We use a 2-layer network with Tanh activation to demonstrate the power of deep learning for solving non-linear problems.\n",
    "\n",
    "**Network Architecture:** Input (2) â†’ Dense(16) â†’ Tanh â†’ Dense(1) â†’ Sigmoid â†’ Output\n",
    "\n",
    "**Training Configuration:**\n",
    "- Learning Rate: 1.0\n",
    "- Epochs: 10,000\n",
    "- Weight Initialization: He initialization\n",
    "- Optimizer: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules and setup\n",
    "import sys\n",
    "for module_name in list(sys.modules.keys()):\n",
    "    if module_name.startswith('lib'):\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "from lib.network import Sequential\n",
    "from lib.layers import Dense\n",
    "from lib.activations import Tanh, Sigmoid\n",
    "from lib.losses import MSE\n",
    "\n",
    "np.random.seed(15)  # For reproducibility\n",
    "\n",
    "# XOR Data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Dense(2, 16))\n",
    "model.add(Tanh())\n",
    "model.add(Dense(16, 1))\n",
    "model.add(Sigmoid())\n",
    "model.use_loss(MSE())\n",
    "\n",
    "print(\"XOR Problem Setup Complete\")\n",
    "print(f\"Input shape: {X.shape}, Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ca197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XOR Network\")\n",
    "print(\"=\" * 50)\n",
    "model.train(X, y, epochs=10000, learning_rate=1.0)\n",
    "print(\"=\" * 50)\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f50d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "pred_values = np.array([p.flatten()[0] for p in predictions])\n",
    "rounded_preds = np.round(pred_values)\n",
    "\n",
    "print(\"\\nRaw Predictions:\")\n",
    "for i, (inp, pred, rounded) in enumerate(zip(X, pred_values, rounded_preds)):\n",
    "    print(f\"  Input {inp} â†’ {pred:.6f} â†’ {int(rounded)}\")\n",
    "\n",
    "print(f\"\\nFinal Prediction Vector: {rounded_preds}\")\n",
    "print(f\"Expected XOR Output:    [0. 1. 1. 0.]\")\n",
    "print(f\"âœ“ MATCH: {np.array_equal(rounded_preds, np.array([0., 1., 1., 0.]))}\")\n",
    "\n",
    "# Calculate final loss\n",
    "final_loss = 0\n",
    "for i in range(len(X)):\n",
    "    output = model.predict([X[i]])[0]\n",
    "    final_loss += model.loss.loss(np.array([[y[i][0]]]), output)\n",
    "final_loss /= len(X)\n",
    "\n",
    "print(f\"\\nFinal Loss: {final_loss:.9f}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecha_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
