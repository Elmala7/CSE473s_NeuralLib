{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaac71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from lib.layers import Dense\n",
    "from lib.activations import Tanh, Sigmoid\n",
    "from lib.losses import MSE\n",
    "from lib.network import Sequential\n",
    "\n",
    "def gradient_check():\n",
    "    print(\"----- Gradient Check -----\")\n",
    "    # Setup\n",
    "    np.random.seed(21)  # For reproducibility\n",
    "    x_sample = np.random.rand(1, 2)\n",
    "    y_sample = np.array([[1]])\n",
    "    layer = Dense(2, 1)\n",
    "    loss_func = MSE()\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    # Get analytic gradient\n",
    "    output = layer.forward(x_sample)\n",
    "    loss_func.loss(y_sample, output)\n",
    "    error_grad = loss_func.loss_prime(y_sample, output)\n",
    "    layer.backward(error_grad, 0)  # learning_rate=0 to avoid weight updates\n",
    "    analytic_grad = layer.dW.copy()\n",
    "\n",
    "    # Get numerical gradient\n",
    "    numerical_grad = np.zeros_like(layer.weights)\n",
    "    it = np.nditer(layer.weights, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        # J(W + e)\n",
    "        original_w = layer.weights[ix]\n",
    "        layer.weights[ix] = original_w + epsilon\n",
    "        output_plus = layer.forward(x_sample)\n",
    "        J_plus = loss_func.loss(y_sample, output_plus)\n",
    "\n",
    "        # J(W - e)\n",
    "        layer.weights[ix] = original_w - epsilon\n",
    "        output_minus = layer.forward(x_sample)\n",
    "        J_minus = loss_func.loss(y_sample, output_minus)\n",
    "        \n",
    "        # (J(W+e) - J(W-e)) / 2e\n",
    "        numerical_grad[ix] = (J_plus - J_minus) / (2 * epsilon)\n",
    "        layer.weights[ix] = original_w  # Restore\n",
    "        it.iternext()\n",
    "    \n",
    "    # Compare\n",
    "    diff = np.linalg.norm(analytic_grad - numerical_grad) / np.linalg.norm(analytic_grad + numerical_grad)\n",
    "    print(f\"Difference: {diff}\")\n",
    "    assert diff < 1e-4, \"Gradient check failed!\"\n",
    "    print(\"Gradient check passed!\")\n",
    "\n",
    "gradient_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules and setup\n",
    "import sys\n",
    "for module_name in list(sys.modules.keys()):\n",
    "    if module_name.startswith('lib'):\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "from lib.network import Sequential\n",
    "from lib.layers import Dense\n",
    "from lib.activations import Tanh, Sigmoid\n",
    "from lib.losses import MSE\n",
    "\n",
    "\n",
    "# XOR Data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Dense(2, 16))\n",
    "model.add(Tanh())\n",
    "model.add(Dense(16, 1))\n",
    "model.add(Sigmoid())\n",
    "model.use_loss(MSE())\n",
    "\n",
    "print(\"XOR Problem Setup Complete\")\n",
    "print(f\"Input shape: {X.shape}, Output shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ca197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XOR Network\")\n",
    "print(\"=\" * 50)\n",
    "model.train(X, y, epochs=10000, learning_rate=1.0)\n",
    "print(\"=\" * 50)\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f50d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "pred_values = np.array([p.flatten()[0] for p in predictions])\n",
    "rounded_preds = np.round(pred_values)\n",
    "\n",
    "print(\"\\nRaw Predictions:\")\n",
    "for i, (inp, pred, rounded) in enumerate(zip(X, pred_values, rounded_preds)):\n",
    "    print(f\"  Input {inp} → {pred:.6f} → {int(rounded)}\")\n",
    "\n",
    "print(f\"\\nFinal Prediction Vector: {rounded_preds}\")\n",
    "print(f\"Expected XOR Output:    [0. 1. 1. 0.]\")\n",
    "print(f\"✓ MATCH: {np.array_equal(rounded_preds, np.array([0., 1., 1., 0.]))}\")\n",
    "\n",
    "# Calculate final loss\n",
    "final_loss = 0\n",
    "for i in range(len(X)):\n",
    "    output = model.predict([X[i]])[0]\n",
    "    final_loss += model.loss.loss(np.array([[y[i][0]]]), output)\n",
    "final_loss /= len(X)\n",
    "\n",
    "print(f\"\\nFinal Loss: {final_loss:.9f}\")\n",
    "print(\"=\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecha_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
